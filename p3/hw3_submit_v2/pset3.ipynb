{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "pset3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtIvNAoYSO5h"
      },
      "source": [
        "# Deep Transition Dependency Parser in PyTorch\n",
        "\n",
        "In this problem set, you will implement a deep transition dependency parser in [PyTorch](https://pytorch.org).\n",
        "\n",
        "You will:\n",
        "\n",
        "- Implement an arc-standard transition-based dependency parser in PyTorch\n",
        "- Implement neural network components for choosing actions and combining stack elements\n",
        "- Train your network to parse English and Norwegian sentences\n",
        "- Implement techniques to improve your parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_9i8dPASO5j"
      },
      "source": [
        "# 0. Setup\n",
        "\n",
        "In order to develop this assignment, you will need [python 3](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [conda](https://docs.conda.io/en/latest/miniconda.html), so a good starting point would be to install that.\n",
        "\n",
        "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
        "- [numpy](https://docs.scipy.org/doc/numpy/user/install.html)\n",
        "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
        "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
        "- [pytorch](https://pytorch.org/get-started/locally/)\n",
        "\n",
        "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldRJevI-SO5k"
      },
      "source": [
        "## About this assignment\n",
        "\n",
        "- This is a Jupyter notebook. You can execute cell blocks by pressing control-enter.\n",
        "- Most of your coding will be in the python source files in the directory ```mynlplib```.\n",
        "- The directory ```tests``` contains unit tests that will be used to grade your assignment, using ```nosetests```. You should run them as you work on the assignment to see that you're on the right track. You are free to look at their source code, if that helps -- though most of the relevant code is also here in this notebook. Learn more about running unit tests at https://nose.readthedocs.io/en/latest/usage.html.\n",
        "- You may want to add more tests, but that is completely optional.\n",
        "- **To submit this assignment, run the script ```make-submission.sh```, and submit the tarball ```pset3-submission.tgz``` on Canvas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVhedMN3SpNv",
        "outputId": "36f921d1-dd30-4e5a-c5d5-e77c5551730a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install nose"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 15.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: nose\n",
            "Successfully installed nose-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSgXgghLSO5l"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as ag\n",
        "\n",
        "import nose\n",
        "import numpy as np\n",
        "\n",
        "from imp import reload"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "VlhEYmorSO5q",
        "outputId": "8d0ad5fd-2d8d-4369-c6a8-eea80fe6efa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('My library versions')\n",
        "\n",
        "print('numpy: {}'.format(np.__version__))\n",
        "print('nose: {}'.format(nose.__version__))\n",
        "print('torch: {}'.format(torch.__version__))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My library versions\n",
            "numpy: 1.18.5\n",
            "nose: 1.3.7\n",
            "torch: 1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWbqYJeUSO5w"
      },
      "source": [
        "To test whether your libraries are the right version, run:\n",
        "\n",
        "`nosetests tests/test_environment.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "QkpJO5RASO5x",
        "outputId": "b6b4cd2f-91ee-4f8c-bcc2-80861e5f11d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# use ! to run shell commands in notebook\n",
        "! nosetests tests/test_environment.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdOmlafISO52"
      },
      "source": [
        "import mynlplib.parsing as parsing\n",
        "import mynlplib.data_tools as data_tools\n",
        "import mynlplib.constants as consts\n",
        "import mynlplib.evaluation as evaluation\n",
        "import mynlplib.utils as utils\n",
        "import mynlplib.feat_extractors as feat_extractors\n",
        "import mynlplib.neural_net as neural_net"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "HcijVs5VSO57"
      },
      "source": [
        "# Read in the datasets\n",
        "reload(data_tools)\n",
        "en_dataset = data_tools.Dataset(consts.EN_TRAIN_FILE, consts.EN_DEV_FILE, consts.EN_TEST_FILE)\n",
        "nr_dataset = data_tools.Dataset(consts.NR_TRAIN_FILE, consts.NR_DEV_FILE, consts.NR_TEST_FILE)\n",
        "\n",
        "# Assign each word a unique index, including two special tokens needed for parsing logic\n",
        "word_to_ix_en = { word: i for i, word in enumerate(en_dataset.vocab) }\n",
        "word_to_ix_nr = { word: i for i, word in enumerate(nr_dataset.vocab) }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcSoVYUcSO5_"
      },
      "source": [
        "# Some constants to keep around\n",
        "LSTM_NUM_LAYERS = 1\n",
        "TEST_EMBEDDING_DIM = 4\n",
        "WORD_EMBEDDING_DIM = 64\n",
        "STACK_EMBEDDING_DIM = 100\n",
        "NUM_FEATURES = 3\n",
        "\n",
        "# Hyperparameters\n",
        "ETA_0 = 0.01\n",
        "DROPOUT = 0.0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTf8WctWSO6I"
      },
      "source": [
        "# High-Level Overview of the Parser\n",
        "Be sure that you have reviewed Eisenstein Ch. 11.3 on transition-based dependency parsing, and are familiar with the relevant terminology.\n",
        "Parsing will proceed as follows:\n",
        "* Initialize your parsing stack and input buffer.\n",
        "* At each step, until the parse is done:\n",
        "  * Extract some features.  We will start with simple features, but these can be anything: words in the sentence, the configuration of the stack, the configuration of the input buffer, the previous action, etc.\n",
        "  * Send these features through a feed-forward (FF) network to get a probability distribution over actions (`SHIFT`, `ARC_L`, `ARC_R`).  The next action you choose is the one with the highest probability.\n",
        "  * If the action is an arc- operation, you use a neural network to combine the two items in the operation and get a dense output to place back on the input buffer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2uhnpjbSO6K"
      },
      "source": [
        "The key classes you will fill in code for are:\n",
        "* Feature extraction in `feat_extractors.py`\n",
        "* The `ParserState` class, which keeps track of the input buffer and parse stack, and offers a public interface for doing the parsing actions to update the state\n",
        "* The `TransitionParser` class, which is a PyTorch module where the core parsing logic resides, in `parsing.py`.\n",
        "* The neural network components in `neural_net.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpWCiWpBSO6L"
      },
      "source": [
        "The network components are compartmentalized as follows:\n",
        "* `TransitionParser`, the base component that contains and coordinates the other substitutable components\n",
        "\n",
        "* Embedding Lookup: You will implement three flavors of embeddings. These embeddings are used to initialize the input buffer, and will be shifted on the stack / serve as inputs to the combiner networks.\n",
        "  - `VanillaWordEmbedding` just gets embeddings from a lookup table.\n",
        "  - `BiLSTMWordEmbedding` will run a sequence model in both directions over the sentence. The hidden state at step t is the embedding for the `t`-th word of the sentence.\n",
        "  - `SuffixAndWordEmbedding` gets embeddings for words as in the vanilla embeddings, and also gets embeddings for word suffixes, and concatenates them together.\n",
        "* Action Choosing: You will implement two action choosing components:\n",
        "  - `FFActionChooser` is a simple feed-forward neural network that outputs log probabilities over the three actions given the extracted features as input.\n",
        "  - `LSTMActionChooser` applies a sequence model that takes the hidden state of the previous action decision as input.\n",
        "\n",
        "* Combiners: You will implement two combiners, which are the network components that take the two embeddings of the items in an arc- operation and creates a single vector.\n",
        "  - `FFCombiner` takes the two input embeddings and gives a dense output.\n",
        "  - `LSTMCombiner` applies a sequence model, where the output embedding is the hidden state of the next timestep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "femj_NJVSO6M"
      },
      "source": [
        "### Parsing example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vice_0PFSO6N"
      },
      "source": [
        "The following is how the input buffer and stack look at each step of a parse, up to the first arc.  The input sentence is \"the dog ran away\".  Our action chooser network takes the top element of the stack, the top element of the input buffer, plus a one-token \"lookahead\" in the input buffer.  $C(x,y)$ refers to calling our combiner network on arguments $x, y$.  Also let $A$ be the set of actions: $\\{ \\text{SHIFT}, \\text{ARC-L}, \\text{ARC-R} \\}$, and let $q_w$ be the embedding for word $w$.\n",
        "\n",
        "1. \n",
        "  * Input Buffer: $\\left[ q_\\text{the}, q_\\text{dog}, q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
        "  * Stack: $\\left[ q_\\text{ROOT} \\right]$\n",
        "  * Action: $ \\text{argmax}_{a \\in A} \\ \\text{ActionChooser}(q_\\text{ROOT}, q_\\text{the}, \\overbrace{q_\\text{dog}}^\\text{lookahead}) \\Rightarrow \\text{SHIFT}$\n",
        "  \n",
        "2.\n",
        "  * Input Buffer: $\\left[ q_\\text{dog}, q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
        "  * Stack: $\\left[ q_\\text{ROOT}, q_\\text{the} \\right]$\n",
        "  * Action: $ \\text{argmax}_{a \\in A} \\ \\text{ActionChooser}(q_\\text{the}, q_\\text{dog}, q_\\text{ran}) \\Rightarrow \\text{ARC-L}$\n",
        "  \n",
        "3.\n",
        "  * Input Buffer: $\\left[C(q_\\text{dog}, q_\\text{the}), q_\\text{ran}, q_\\text{away}, q_\\text{END-INPUT} \\right]$\n",
        "  * Stack: $\\left[ q_\\text{ROOT} \\right]$\n",
        "  \n",
        "This is a partial picture of parsing - we keep more than just the embedding on the stack and input buffer.  We also keep the word and its position in the sentence so that when we create an arc, we know what edge was just created.\n",
        "So, for example, the initial input buffer really looks like\n",
        "\n",
        "$$ \\left[ (\\text{the}, 0, q_\\text{the}), (\\text{dog}, 1, q_\\text{dog}), (\\text{ran}, 2, q_\\text{ran}), (\\text{away}, 3, q_\\text{away}), (\\text{END-INPUT}, 4, q_\\text{END-INPUT}) \\right] $$\n",
        "\n",
        "Before beginning, I recommend completing the parse by hand, drawing the input buffer and stack at each step, and explicity listing the arguments to the action chooser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLz3fyg5SO6N"
      },
      "source": [
        "# 1. Managing and Updating the Parser State (12 points)\n",
        "\n",
        "In this part of the assignment, you will work with the ParserState class, that keeps track of the parser's input buffer and stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9TvEu1HSO6O"
      },
      "source": [
        "### Deliverable 1.1: Implementing Arc\n",
        "\n",
        "#### 1.1a: Get arc components (2 points)\n",
        "You will implement the generalized arc- operation of the `ParserState` in `parsing.py`, in the function `_arc`, in two parts.\n",
        "\n",
        "First, fill in `_get_arc_components` in `parsing.py` to select the head and modifier according to the action passed in. This method should also remove the items from the stack and input buffer.\n",
        "Arc actions follow the arc-standard procedure, from Eisenstein Ch. 11.3.1.\n",
        "\n",
        "- **Test**: ` test_parser.py:test_get_arc_components_d1_1a`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0UnsLCYSO6P"
      },
      "source": [
        "reload(parsing)\n",
        "test_sentence = \"The man ran away\".split()\n",
        "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
        "                                   [None] * (len(test_sentence)+1),\n",
        "                                   utils.DummyCombiner())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true,
        "id": "VwyAA9RxSO6T",
        "outputId": "bdf0d816-e5b4-41cb-f0d3-d21fee4cf99d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "parser_state.shift()\n",
        "parser_state.shift()\n",
        "print(parser_state)\n",
        "\n",
        "head, modifier = parser_state._get_arc_components(consts.Actions.ARC_L)\n",
        "print(head, modifier)\n",
        "\n",
        "head, modifier = parser_state._get_arc_components(consts.Actions.ARC_R)\n",
        "print(head, modifier)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stack: ['<ROOT>', 'The', 'man']\n",
            "Input Buffer: ['ran', 'away', '<END-OF-INPUT>']\n",
            "\n",
            "StackEntry(headword='ran', headword_pos=2, embedding=None) StackEntry(headword='man', headword_pos=1, embedding=None)\n",
            "StackEntry(headword='The', headword_pos=0, embedding=None) StackEntry(headword='away', headword_pos=3, embedding=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI3shxrp89F4",
        "outputId": "38c25729-2eec-4890-f140-88e7e057ccdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_get_arc_components_d1_1a"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RWC_uQESO6Y"
      },
      "source": [
        "#### 1.1b: Create the arc (2 points)\n",
        "Now, fill in `_create_arc` in `parsing.py` to use the `ParserState`'s `combiner` component to **combine** the passed in head and modifier, put the combination on the input buffer, and create a new dependency graph edge. At this point, we are just using a dummy combiner so we can test the logic.\n",
        "\n",
        "You will want to familiarize yourself with the `StackEntry` and `DepGraphEdge` objects used by the `ParserState` object for this one.\n",
        "\n",
        "- **Test**: ` test_parser.py:test_create_arc_d1_1b`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "HvUkxXAoSO6Z",
        "outputId": "c6a07938-e7b7-44d4-cdd0-1cdefbd21ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(parsing)\n",
        "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
        "                                   [None] * (len(test_sentence)+1),\n",
        "                                   utils.DummyCombiner())\n",
        "\n",
        "print(parser_state)\n",
        "\n",
        "parser_state.shift()\n",
        "print(parser_state)\n",
        "\n",
        "arc = parser_state.arc_left()\n",
        "print(\"First arc: Head: {}, Modifier: {}\".format(arc[0], arc[1]), \"\\n\")\n",
        "print(parser_state)\n",
        "\n",
        "parser_state.shift()\n",
        "arc = parser_state.arc_left()\n",
        "print(\"Second arc: Head: {}, Modifier: {}\".format(arc[0], arc[1]), \"\\n\")\n",
        "print(parser_state)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stack: ['<ROOT>']\n",
            "Input Buffer: ['The', 'man', 'ran', 'away', '<END-OF-INPUT>']\n",
            "\n",
            "Stack: ['<ROOT>', 'The']\n",
            "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
            "\n",
            "First arc: Head: ('man', 1), Modifier: ('The', 0) \n",
            "\n",
            "Stack: ['<ROOT>']\n",
            "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
            "\n",
            "Second arc: Head: ('ran', 2), Modifier: ('man', 1) \n",
            "\n",
            "Stack: ['<ROOT>']\n",
            "Input Buffer: ['ran', 'away', '<END-OF-INPUT>']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5_8AMmhBXpt",
        "outputId": "6628bd21-ec94-40d5-eb4d-f4451879b19b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_create_arc_d1_1b"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBYYiEkLSO6e"
      },
      "source": [
        "### Deliverable 1.2: Parser Terminating Condition (4 points)\n",
        "In this short (one line) deliverable, implement `done_parsing()` in `ParserState`.  Think about what the input buffer and stack look like at the end of a parse.\n",
        "\n",
        "- **Test**: `test_parsing.py:test_stack_terminating_cond_d1_2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "7bydzdkDSO6f",
        "outputId": "d5383f48-66ff-4490-91b1-fa2f21606ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(parsing)\n",
        "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
        "                                   [None] * (len(test_sentence)+1),\n",
        "                                   utils.DummyCombiner())\n",
        "\n",
        "parser_state.shift()\n",
        "parser_state.arc_left()\n",
        "parser_state.shift()\n",
        "parser_state.arc_left()\n",
        "\n",
        "print(parser_state.done_parsing())\n",
        "\n",
        "parser_state.shift()\n",
        "parser_state.arc_right()\n",
        "print(parser_state.done_parsing())\n",
        "\n",
        "parser_state.arc_right()\n",
        "print(parser_state.done_parsing())\n",
        "\n",
        "parser_state.shift()\n",
        "print(parser_state.done_parsing())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yijsVq7VGYOG",
        "outputId": "b21b6d4f-7ec4-45a1-db5a-ce0929171699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_stack_terminating_cond_d1_2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zEq4QlYSO6j"
      },
      "source": [
        "### Deliverable 1.3: Validating parser actions (4 points)\n",
        "Implement the `_validate_action` method in `parsing.TransitionParser`. This will be used in the prediction setting, when the gold standard is not available. We need to ensure that any action we take is legal. Here are the rules:\n",
        "\n",
        "- You cannot shift when the input buffer has <= 2 items on it (including the end of input token), UNLESS the stack is empty.\n",
        "  - **In this case, do `ARC_R` by default.**\n",
        "- You cannot do an arc- operation when the stack is empty (this will happen after creating an arc with ROOT).\n",
        "  - **In this case, do `SHIFT` by default.**\n",
        "- You cannot do an arc-left operation when the root token is on top of the stack.\n",
        "  - **In this case, do `SHIFT` or `ARC-R` depending on the state of the input buffer.**\n",
        "  \n",
        "**Test:**\n",
        "- `test_parser.py:test_validate_action_d1_3`\n",
        "\n",
        "**Make sure you pass the test before you move on. The code blocks below are not meant to be comprehensive.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hckIZLESO6k"
      },
      "source": [
        "reload(parsing)\n",
        "parser_state = parsing.ParserState(test_sentence + [consts.END_OF_INPUT_TOK], \n",
        "                                   [None] * (len(test_sentence)+1),\n",
        "                                   utils.DummyCombiner())\n",
        "ix_to_action = consts.Actions.ix_to_action"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "l7mFJHTQSO6o",
        "outputId": "59327c96-486b-4522-e2b1-95fc65a41cd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(parser_state)\n",
        "act_to_do = consts.Actions.ARC_L\n",
        "valid_action = parser_state._validate_action(act_to_do)\n",
        "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))\n",
        "\n",
        "parser_state.shift()\n",
        "\n",
        "print(parser_state)\n",
        "act_to_do = consts.Actions.ARC_L\n",
        "valid_action = parser_state._validate_action(act_to_do)\n",
        "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))\n",
        "\n",
        "parser_state.shift()\n",
        "parser_state.shift()\n",
        "\n",
        "print(parser_state)\n",
        "act_to_do = consts.Actions.SHIFT\n",
        "valid_action = parser_state._validate_action(act_to_do)\n",
        "print(\"Chosen action: %s, Valid action: %s\\n\" % (ix_to_action[act_to_do], ix_to_action[valid_action]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stack: ['<ROOT>']\n",
            "Input Buffer: ['The', 'man', 'ran', 'away', '<END-OF-INPUT>']\n",
            "\n",
            "Chosen action: ARC_L, Valid action: SHIFT\n",
            "\n",
            "Stack: ['<ROOT>', 'The']\n",
            "Input Buffer: ['man', 'ran', 'away', '<END-OF-INPUT>']\n",
            "\n",
            "Chosen action: ARC_L, Valid action: ARC_L\n",
            "\n",
            "Stack: ['<ROOT>', 'The', 'man', 'ran']\n",
            "Input Buffer: ['away', '<END-OF-INPUT>']\n",
            "\n",
            "Chosen action: SHIFT, Valid action: ARC_R\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYFqyxU-QBLe",
        "outputId": "a611ab1b-64d4-413e-f28d-3a9f4fa2a0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_validate_action_d1_3"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCmssyEGSO6s"
      },
      "source": [
        "# 2. Neural Network for Action Decisions (12 points)\n",
        "In this part of the assignment, you will use PyTorch to create a neural network which examines the current state of the parse and makes the decision to either shift, arc left, or arc right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csvFmzxSO6u"
      },
      "source": [
        "### Deliverable 2.1: Word Embedding Lookup (3 points)\n",
        "Implement the class `VanillaWordEmbedding` in `neural_net.py`\n",
        "([Docs for Pytorch embeddings](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html))\n",
        "\n",
        "Hint: You will have to turn the input, which is a list of strings (the words in the sentence), into a format that your embedding lookup table can take. \n",
        "\n",
        "**Test:** `test_parser.py:test_word_embed_lookup_d2_1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "TfKr02SnSO6v",
        "outputId": "92079950-2fa0-49cc-c6ab-d5f77caf5797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(neural_net)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "test_sentence = \"natural language processing\".split()\n",
        "test_word_to_ix = { \"natural\": 0, \"language\": 1, \"processing\": 2 }\n",
        "\n",
        "word_embedder = neural_net.VanillaWordEmbedding(test_word_to_ix, TEST_EMBEDDING_DIM)\n",
        "embeds = word_embedder(test_sentence)\n",
        "print(type(embeds))\n",
        "print(len(embeds), \"\\n\")\n",
        "print(\"Embedding for 'natural':\\n {}\".format(embeds[0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "3 \n",
            "\n",
            "Embedding for 'natural':\n",
            " tensor([[0.6614, 0.2669, 0.0617, 0.6213]], grad_fn=<ViewBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkASsILvkAHG",
        "outputId": "4eab9fc4-18b7-4a73-a353-0441806e43e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embeds[0].size()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StaPJXrkfTR0",
        "outputId": "641037cb-c10f-420d-a47e-a687ea27b7fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_word_embed_lookup_d2_1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.005s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlJE3C9eSO6z"
      },
      "source": [
        "### Deliverable 2.2: Feature Extraction (3 points)\n",
        "Fill in the `SimpleFeatureExtractor` class in `feat_extractors.py` to give the following 3 features as a list **in this order**:\n",
        "* The embedding of the top of the stack\n",
        "* The embedding of the first token in the input buffer\n",
        "* The embedding of the next token in the input buffer (one-token lookahead)\n",
        "\n",
        "If at this point you have not poked around `ParserState` to see how it stores the state, now would be a good time.\n",
        "\n",
        "**Test:** `test_parser.py:test_feature_extraction_d2_2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "AFAdTcLjSO60",
        "outputId": "4b9f9b97-9e08-4b50-8ebe-d2bc6decb1d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(feat_extractors)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "test_sentence = \"The Sound and the Fury\".split()\n",
        "test_word_to_ix = { word: i for i, word in enumerate(sorted(set(test_sentence))) }\n",
        "\n",
        "embedder = neural_net.VanillaWordEmbedding(test_word_to_ix, TEST_EMBEDDING_DIM)\n",
        "embeds = embedder(test_sentence)\n",
        "\n",
        "state = parsing.ParserState(test_sentence, embeds, utils.DummyCombiner())\n",
        "\n",
        "state.shift()\n",
        "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
        "feats = feat_extractor.get_features(state)\n",
        "\n",
        "print(\"Embedding for 'The':\\n {}\".format(feats[0]))\n",
        "print(\"Embedding for 'Sound':\\n {}\".format(feats[1]))\n",
        "print(\"Embedding for 'and' (from buffer lookahead):\\n {}\".format(feats[2]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding for 'The':\n",
            " tensor([[ 0.4391,  1.1712,  1.7674, -0.0954]], grad_fn=<ViewBackward>)\n",
            "Embedding for 'Sound':\n",
            " tensor([[ 0.8657,  0.2444, -0.6629,  0.8073]], grad_fn=<ViewBackward>)\n",
            "Embedding for 'and' (from buffer lookahead):\n",
            " tensor([[ 0.0612, -0.6177, -0.7981, -0.1316]], grad_fn=<ViewBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZHAbgGpu29n",
        "outputId": "55bafd74-df37-49cb-b56b-b93b47b04ce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_feature_extraction_d2_2"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.002s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68P4osrTSO64"
      },
      "source": [
        "### Deliverable 2.3: Feedforward Network for Choosing Actions (3 points)\n",
        "Implement the class `neural_net.FFActionChooser` according to the specification.\n",
        "You will need to take the list of embeddings passed in (that come from your feature extractor) and concatenate them to one long row vector (size [1 x num actions])\n",
        "\n",
        "This network takes as input the features from your feature extractor, concatenates them, runs them through a feedforward network, and outputs log probabilities over actions.\n",
        "\n",
        "**Test:** `test_parser.py:test_action_chooser_d2_3`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "sRo8p7C5SO65",
        "outputId": "a2901e19-6e09-4b6b-f05d-e1953d88705a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(neural_net)\n",
        "torch.manual_seed(1)\n",
        "act_chooser = neural_net.FFActionChooser(TEST_EMBEDDING_DIM * NUM_FEATURES)\n",
        "feats = [ ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM)) for _ in range(NUM_FEATURES) ] # make some dummy feature embeddings\n",
        "log_probs = act_chooser(feats)\n",
        "print(log_probs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input dim:  12\n",
            "tensor([-1.2443, -0.8323, -1.2844], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL_5I4R97otp",
        "outputId": "4474fa6e-4515-4d86-81a6-84557839f39b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_action_chooser_d2_3"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.003s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvuWeWkzSO69"
      },
      "source": [
        "### Deliverable 2.4: Network for Combining Stack Items (3 points)\n",
        "Implement the class `neural_net.FFCombiner` according to the specification. \n",
        "Recall that what this component does is take two embeddings, the head and modifier, during an arc- operation and output a combined embedding (of size [1 x embedding_dim]), which is then pushed back onto the input buffer during parsing.\n",
        "\n",
        "**Test:** `test_parser.py:test_combiner_d2_4`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "mi527OxjSO6-",
        "outputId": "7adc5bb2-1a83-4b96-964c-ae928db2332f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(neural_net)\n",
        "torch.manual_seed(1)\n",
        "combiner = neural_net.FFCombiner(TEST_EMBEDDING_DIM)\n",
        "\n",
        "# Again, make dummy inputs\n",
        "head_feat = ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM))\n",
        "modifier_feat = ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM))\n",
        "combined = combiner(head_feat, modifier_feat)\n",
        "print(combined)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4285, -0.1363,  0.4046,  0.6006]], grad_fn=<ViewBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLLyl46hpN2k",
        "outputId": "565ee5da-63fc-4e3a-bcca-cf9b2f0b89b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_combiner_d2_4"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.004s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuAljcVrSO7D"
      },
      "source": [
        "# 3. Return of the Parser (12 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtM056LxSO7E"
      },
      "source": [
        "### Deliverable 3.1: Parser Training Code (8 points)\n",
        "We will now complete the parser and train it on our data. It is important to understand the difference between the following tasks:\n",
        "\n",
        "* Training: Training the model involves passing it sentences along with the correct sequence of actions, and updating weights.\n",
        "* Evaluation: We can evaluate the parser by passing it sentences along with the correct sequence of actions, and see how many actions it predicts correctly.  This is identical to training, except the weights are not updated after making a prediction.\n",
        "* Prediction: After setting the weights, we give it a raw sentence (no gold-standard actions), and let it follow its own predicted actions to create a dependency graph, which we can compare to the ground truth.\n",
        "\n",
        "You will implement the `forward()` function in `mynlplib.parsing.TransitionParser`.\n",
        "\n",
        "At this point, it is necessary to have all of the components from part 2 in place for constructing the parser.\n",
        "\n",
        "The parsing logic is roughly as follows:\n",
        "* Loop until parsing state is in its terminating state (deliverable 1.2)\n",
        "* Get the features from the parsing state (deliverable 2.2)\n",
        "* Send them through your action chooser network to get log probabilities over actions (deliverable 2.3)\n",
        "* If you have `gold_actions`, do them. Otherwise (when predicting), take the argmax of your log probabilities, validate the action (deliverable 1.3), and do that. An argmax function is provided for you in `utils.argmax`.\n",
        "\n",
        "Make sure to keep track of the things that the function wants to keep track of\n",
        "* Do all of your actions by calling the appropriate function on your `parser_state`\n",
        "* Append each output autograd.Variable from your action_chooser to the outputs list\n",
        "* Append each action you do to `actions_done`\n",
        "* Build the set of dependency edges as you go\n",
        "\n",
        "**Tests:**\n",
        "- `test_parser.py:test_parse_logic_d3_1`\n",
        "- `test_parser.py:test_predict_after_train_d3_1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDB9lWBaSO7F"
      },
      "source": [
        "test_sentence = \"The man ran away\".split()\n",
        "test_word_to_ix = { word: i for i, word in enumerate(sorted(set(test_sentence))) }\n",
        "test_word_to_ix[consts.END_OF_INPUT_TOK] = len(test_word_to_ix)\n",
        "test_sentence_vocab = set(test_sentence)\n",
        "gold_actions = [\"SHIFT\", \"ARC_L\", \"SHIFT\", \"ARC_L\", \"SHIFT\", \"ARC_R\", \"ARC_R\", \"SHIFT\"]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true,
        "id": "RKhZDtkvSO7J",
        "outputId": "22733730-ada9-47b4-db16-91f10ebe0b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(parsing)\n",
        "torch.manual_seed(1)\n",
        "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
        "word_embedding_lookup = neural_net.VanillaWordEmbedding(test_word_to_ix, STACK_EMBEDDING_DIM)\n",
        "action_chooser = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
        "combiner_network = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
        "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
        "                                     action_chooser, combiner_network)\n",
        "output, depgraph, actions_done = parser(test_sentence, gold_actions)\n",
        "print(depgraph)\n",
        "print(actions_done)\n",
        "print(output)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input dim:  300\n",
            "{DepGraphEdge(head=('ran', 2), modifier=('away', 3)), DepGraphEdge(head=('<ROOT>', -1), modifier=('ran', 2)), DepGraphEdge(head=('ran', 2), modifier=('man', 1)), DepGraphEdge(head=('man', 1), modifier=('The', 0))}\n",
            "[0, 1, 0, 1, 0, 2, 2, 0]\n",
            "[tensor([[-0.9461, -1.2026, -1.1669]], grad_fn=<ViewBackward>), tensor([[-1.1964, -0.9848, -1.1263]], grad_fn=<ViewBackward>), tensor([[-1.0913, -1.2777, -0.9531]], grad_fn=<ViewBackward>), tensor([[-1.1162, -1.0229, -1.1618]], grad_fn=<ViewBackward>), tensor([[-1.1921, -1.0527, -1.0572]], grad_fn=<ViewBackward>), tensor([[-1.1376, -1.0887, -1.0707]], grad_fn=<ViewBackward>), tensor([[-1.0584, -1.1551, -1.0847]], grad_fn=<ViewBackward>), tensor([[-1.1147, -1.2839, -0.9288]], grad_fn=<ViewBackward>)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6VUjDzqUTDQ"
      },
      "source": [
        "#{DepGraphEdge(head=('<ROOT>', -1), modifier=('ran', 2)), DepGraphEdge(head=('ran', 2), modifier=('man', 1)), DepGraphEdge(head=('man', 1), modifier=('The', 0)), DepGraphEdge(head=('ran', 2), modifier=('away', 3))}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAKu_19FUze3"
      },
      "source": [
        "#output[0][0]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2QsJR-QzYFD",
        "outputId": "5d6a21bf-b23f-429c-ef3d-14a29d5ae4a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_parse_logic_d3_1"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.007s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMa14kS8zin9",
        "outputId": "4cdff8b3-6e62-4513-8903-e550729150fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_predict_after_train_d3_1"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.981s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od4dZOiySO7N"
      },
      "source": [
        "### Now Train the Parser!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmW3q-dxSO7O"
      },
      "source": [
        "Training your parser may take some time. There are 10,000 training sentences. We take a subset of them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRggU6caSO7P"
      },
      "source": [
        "def train_parser(parser, optimizer, dataset, n_epochs=1, n_train_insts=1000):\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {}\".format(epoch+1))\n",
        "\n",
        "        parser.train() # turn on dropout layers if they are there\n",
        "        parsing.train(dataset.training_data[:n_train_insts], parser, optimizer, verbose=True)\n",
        "\n",
        "        print(\"Dev Evaluation\")\n",
        "        parser.eval() # turn them off for evaluation\n",
        "        parsing.evaluate(dataset.dev_data, parser, verbose=True)\n",
        "        print(\"F-Score: {}\".format(evaluation.compute_metric(parser, dataset.dev_data, evaluation.fscore)))\n",
        "        print(\"Attachment Score: {}\".format(evaluation.compute_attachment(parser, dataset.dev_data)))\n",
        "        print(\"\\n\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPpOl_N-SO7S",
        "outputId": "1bfe3978-96e0-4e0f-9bd0-e19aaadfccf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(parsing)\n",
        "torch.manual_seed(1)\n",
        "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
        "word_embedding_lookup = neural_net.VanillaWordEmbedding(word_to_ix_en, STACK_EMBEDDING_DIM)\n",
        "action_chooser = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
        "combiner_network = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
        "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
        "                                     action_chooser, combiner_network)\n",
        "optimizer = optim.SGD(parser.parameters(), lr=ETA_0)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input dim:  300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "7OhSGBPKSO7W",
        "outputId": "f31cf43f-2193-4ee6-c4ad-e43d6e004fb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "torch.manual_seed(1)\n",
        "parsing.train(en_dataset.training_data[:100], parser, optimizer, verbose=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of instances: 100    Number of network actions: 4836\n",
            "Acc: 0.7001654259718776  Loss: 33.50165307641029\n",
            "Number of instances: 100    Number of network actions: 4836\n",
            "Acc: 0.8339536807278742  Loss: 19.933889707326887\n",
            "Number of instances: 100    Number of network actions: 4836\n",
            "Acc: 0.9032258064516129  Loss: 11.863574793338776\n",
            "Number of instances: 100    Number of network actions: 4836\n",
            "Acc: 0.9427212572373863  Loss: 7.174399619698525\n",
            "1 loop, best of 3: 4.15 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "qVBd_wYnSO7b",
        "outputId": "44fceb64-011c-408f-f9cd-dc46bdaa2771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train the parser for a while here.\n",
        "# Shouldn't take *too* long, even on a laptop\n",
        "torch.manual_seed(1)\n",
        "train_parser(parser, optimizer, en_dataset, n_train_insts=1000)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of instances: 1000    Number of network actions: 44560\n",
            "Acc: 0.8212073608617594  Loss: 20.35936585931387\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.8297993184399849  Loss: 15.161761924960908\n",
            "F-Score: 0.48456278370272565\n",
            "Attachment Score: 0.4715385586267828\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjMFaCOYSO7f"
      },
      "source": [
        "### Deliverable 3.2: Test Data Predictions (2 points)\n",
        "Run the code below to output your predictions on the test data and dev data.  You can run the dev test to verify you are correct up to this point.  The test data evaluation is for us.\n",
        "\n",
        "**Test**: `test_parser.py:test_dev_d3_2_english`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NneGgIDmSO7g",
        "outputId": "242ba157-1772-447c-aa8c-59ee1009f40a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
        "evaluation.output_preds(consts.EN_D3_2_DEV_FILENAME, parser, dev_sentences)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ekesMLtSO7k",
        "outputId": "fc46df95-8b3a-482f-e3e3-ca5cac37e5c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluation.output_preds(consts.EN_D3_2_TEST_FILENAME, parser, en_dataset.test_data)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB1_d11Im_hb",
        "outputId": "622468a9-264c-424e-8ff5-35b3d5ac3238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_dev_d3_2_english"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.008s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QFqfKZDSO7o"
      },
      "source": [
        "### Deliverable 3.3: Dependency parsing in Norwegian (2 points)\n",
        "Run the code below to output your predictions on the **norwegian** test data and dev data.  You can run the dev test to verify you are correct up to this point.  The test data evaluation is for us.\n",
        "\n",
        "**Test**: `test_parser.py:test_dev_d3_3_norwegian`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27WwPsGKSO7o",
        "outputId": "6671574b-dc56-4485-ff70-748826902a0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(parsing)\n",
        "torch.manual_seed(1)\n",
        "feat_extractor_nr = feat_extractors.SimpleFeatureExtractor()\n",
        "word_embedding_lookup_nr = neural_net.VanillaWordEmbedding(word_to_ix_nr, STACK_EMBEDDING_DIM)\n",
        "action_chooser_nr = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
        "combiner_network_nr = neural_net.FFCombiner(STACK_EMBEDDING_DIM)\n",
        "parser_nr = parsing.TransitionParser(feat_extractor_nr, word_embedding_lookup_nr,\n",
        "                                     action_chooser_nr, combiner_network_nr)\n",
        "optimizer_nr = optim.SGD(parser_nr.parameters(), lr=ETA_0)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input dim:  300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "RCUeBPQhSO7s",
        "outputId": "1a50cd69-b9d8-4328-8469-0912a496bdd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.manual_seed(1)\n",
        "train_parser(parser_nr, optimizer_nr, nr_dataset, n_epochs=2, n_train_insts=1000)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of instances: 1000    Number of network actions: 30942\n",
            "Acc: 0.8083511085256286  Loss: 14.134146607089788\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 16028\n",
            "Acc: 0.8147616670826054  Loss: 14.839868331264832\n",
            "F-Score: 0.4623923418123258\n",
            "Attachment Score: 0.4363613676066883\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Number of instances: 1000    Number of network actions: 30942\n",
            "Acc: 0.874313231206774  Loss: 9.564662385477684\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 16028\n",
            "Acc: 0.8149488395308211  Loss: 16.840433283043495\n",
            "F-Score: 0.4751974626729133\n",
            "Attachment Score: 0.4469678063389069\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MBUH8k8ZSO7w",
        "outputId": "d05b4619-eb9d-4e51-d399-2f6e0dad43b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(evaluation)\n",
        "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
        "evaluation.output_preds(consts.NR_D3_3_DEV_FILENAME, parser_nr, dev_sentences_nr)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJiX8fcSSO71",
        "outputId": "96764651-e9db-4e58-e648-bc02cffbe0ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluation.output_preds(consts.NR_D3_3_TEST_FILENAME, parser_nr, nr_dataset.test_data)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEkZRwnWnsc1",
        "outputId": "3333e278-f42e-46d2-8acb-775d6ab80813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_dev_d3_3_norwegian"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.009s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PW2at5nSO76"
      },
      "source": [
        "# 4. Evaluation and Training Improvements (18 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cDVLIIRSO77"
      },
      "source": [
        "### Deliverable 4.1: BiLSTM Word Embeddings (3 points)\n",
        "Implement the class `BiLSTMWordEmbedding` in `neural_net.py`.\n",
        "This class can replace your `VanillaWordEmbedding`.\n",
        "This class implements a sequence model over the sentence, where the t'th word's embedding is the hidden state at timestep t.\n",
        "This means that, rather than have our embeddings on the stack only include the semantics of a single word, our embeddings will contain information from all parts of the sentence (the LSTM will, in principle, learn what information is relevant).\n",
        "\n",
        "**Test**: `tests/test_parser.py:test_bilstm_word_embeds_d4_1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "U9uO0lJ2SO78",
        "outputId": "1eba91d2-be98-413f-b27b-1da55812607c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(neural_net)\n",
        "torch.manual_seed(1)\n",
        "test_sentence = \"Noam Chomsky\".split()\n",
        "test_word_to_ix = { \"Noam\": 0, \"Chomsky\": 1 }\n",
        "\n",
        "lstm_word_embedder = neural_net.BiLSTMWordEmbedding(test_word_to_ix,\n",
        "                                                    WORD_EMBEDDING_DIM,\n",
        "                                                    STACK_EMBEDDING_DIM,\n",
        "                                                    num_layers=LSTM_NUM_LAYERS,\n",
        "                                                    dropout=DROPOUT)\n",
        "    \n",
        "lstm_embeds = lstm_word_embedder(test_sentence)\n",
        "print(type(lstm_embeds))\n",
        "print(len(lstm_embeds), \"\\n\")\n",
        "print(\"Embedding for Noam:\\n {}\".format(lstm_embeds[0]))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "2 \n",
            "\n",
            "Embedding for Noam:\n",
            " tensor([[-6.3007e-02,  2.4330e-01, -7.0760e-02, -1.1852e-01,  1.8881e-01,\n",
            "         -1.6543e-01, -2.1600e-02,  1.4040e-02, -6.8058e-02, -1.8666e-01,\n",
            "          1.0207e-01,  2.2894e-02, -5.8540e-02, -6.3337e-02, -2.9607e-01,\n",
            "         -2.0053e-02, -1.8389e-01, -9.1271e-02, -5.1386e-02, -3.4879e-01,\n",
            "         -3.8826e-02,  8.8795e-02, -3.8836e-02,  1.2170e-02,  4.6013e-02,\n",
            "         -1.3923e-01,  1.9091e-02,  7.1751e-02,  9.5653e-02, -3.5629e-01,\n",
            "          1.9788e-01,  2.9786e-02,  6.1633e-02,  4.7286e-02, -2.9223e-01,\n",
            "         -7.4602e-02,  2.4812e-01, -1.3309e-01,  4.2635e-02,  4.2023e-02,\n",
            "          3.1180e-02,  5.5482e-03, -1.1297e-01,  1.4215e-02, -1.0769e-01,\n",
            "         -1.4725e-01, -7.3080e-02,  2.1588e-02,  1.7645e-01,  4.3659e-02,\n",
            "         -2.4070e-04,  1.1204e-02, -2.2866e-01,  1.1086e-01, -3.3928e-02,\n",
            "         -1.3846e-01, -8.5202e-03,  8.6117e-02,  9.5097e-02, -1.2923e-01,\n",
            "         -2.7905e-03, -6.9797e-02,  1.6902e-01, -1.0969e-01, -1.3452e-01,\n",
            "          1.5670e-01,  7.2735e-02, -2.0805e-01, -1.5710e-01, -1.4324e-01,\n",
            "         -7.5958e-02,  1.8515e-01,  4.4450e-02,  7.9908e-02, -1.2914e-01,\n",
            "         -1.5535e-01,  3.2856e-02, -1.4755e-01,  5.6925e-02, -9.1804e-02,\n",
            "         -1.3480e-01, -1.8542e-02,  1.4489e-01,  9.3584e-02,  6.2046e-02,\n",
            "         -9.7762e-02,  8.8054e-02,  6.1904e-02,  1.5637e-02,  5.7491e-02,\n",
            "          3.4077e-02, -1.6989e-01,  3.2679e-03, -1.2580e-01,  7.0331e-02,\n",
            "         -9.3819e-02, -1.1116e-01, -1.8796e-01,  3.5303e-02, -1.0488e-01]],\n",
            "       grad_fn=<ExpandBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5NzDrnUI3vB",
        "outputId": "142c3b93-a48e-466a-940f-f34693b758fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_bilstm_word_embeds_d4_1"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.005s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGSJjfJDSO8A"
      },
      "source": [
        "### Deliverable 4.2: Suffix Embeddings (3 points)\n",
        "We can also try to more explicitly include morphological information by embedding the suffix of a word in addition to the word itself. We approximate the \"suffix\" by just looking at the last two characters of a word.\n",
        "\n",
        "First, implement the function `build_suff_to_ix` in `utils.py`. It should take in a `word_to_ix` lookup and return a `suff_to_ix` lookup.\n",
        "\n",
        "Then, implement the class `SuffixAndWordEmbedding` in `neural_net.py`.\n",
        "This class embeds the words and suffixes in a sentence and then concatenates them to form one embedding. \n",
        "\n",
        "**Test**: `tests/test_parser.py:test_suff_word_embeds_d4_2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "_g5A71iQSO8B"
      },
      "source": [
        "reload(utils)\n",
        "suff_to_ix_en = utils.build_suff_to_ix(word_to_ix_en)\n",
        "suff_to_ix_nr = utils.build_suff_to_ix(word_to_ix_nr)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "v40hsnYNSO8G",
        "outputId": "ade8c8ce-fd10-4197-8549-5d0080945857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(suff_to_ix_en), len(suff_to_ix_nr)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1145, 849)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ysgRvZoSO8N",
        "outputId": "012c9808-103f-4e03-b674-1d7b4e8e4cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(neural_net)\n",
        "torch.manual_seed(1)\n",
        "test_sentence = \"prefix fixsuf fixinfix\".split()\n",
        "test_word_to_ix = { \"prefix\": 0, \"fixsuf\": 1, \"fixinfix\": 2 }\n",
        "test_suff_to_ix = utils.build_suff_to_ix(test_word_to_ix)\n",
        "print(test_suff_to_ix)\n",
        "suff_word_embedder = neural_net.SuffixAndWordEmbedding(test_word_to_ix, test_suff_to_ix, TEST_EMBEDDING_DIM)\n",
        "test_embs = suff_word_embedder(test_sentence)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ix': 0, 'uf': 1}\n",
            "tensor([[0],\n",
            "        [1],\n",
            "        [0]])\n",
            "tensor([[0],\n",
            "        [1],\n",
            "        [2]])\n",
            "tensor([[ 0.6614,  0.2669, -1.5228,  0.3817]], grad_fn=<ViewBackward>)\n",
            "tensor([[ 0.0617,  0.6213, -1.0276, -0.5631]], grad_fn=<ViewBackward>)\n",
            "tensor([[-0.4519, -0.1661, -1.5228,  0.3817]], grad_fn=<ViewBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uwVnttcQ5YR"
      },
      "source": [
        "#[ 0.6614,  0.2669, -1.5228,  0.3817]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true,
        "id": "CF4IwBuTSO8Q",
        "outputId": "1f8aaaa5-01ee-476c-f83f-a5cb6cea82a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_embs[0]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6614,  0.2669, -1.5228,  0.3817]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jmd-vaLORwt",
        "outputId": "7a3481be-d08e-4485-afda-44048ec40db2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_suff_word_embeds_d4_2"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.007s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTdOcyKySO8T"
      },
      "source": [
        "### Deliverable 4.3: Pretrained Embeddings (2 points)\n",
        "\n",
        "Fill in the function `initialize_with_pretrained` in `utils.py`.\n",
        "\n",
        "It will take a word embedding lookup component and initialize its lookup table with pretrained embeddings, which are provided. Note that this is only applicable for the Vanilla, BiLSTM, and SuffixAndWord embedding components.\n",
        "\n",
        "**Test**: `tests/test_parser.py:test_pretrained_embeddings_d4_3`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "7HG-FHsBSO8U",
        "outputId": "08757eb4-455f-471f-f6aa-202f734495bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pickle\n",
        "pretrained_embeds = pickle.load(open(consts.PRETRAINED_EMBEDS_FILE, 'rb'))\n",
        "print(pretrained_embeds['four'][:5])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.12429751455783844, -0.11472601443529129, -0.5684014558792114, -0.396965891122818, 0.22938089072704315]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkeK-HRkSO8Y"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "embedder = neural_net.VanillaWordEmbedding(word_to_ix_en, 64)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Feyjp-wk_oO",
        "outputId": "cd539074-e9af-466e-fedd-5a15d8c1dfff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(neural_net)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'mynlplib.neural_net' from '/content/mynlplib/neural_net.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "vfPxCjSbSO8h",
        "outputId": "b0635c0c-e54d-49d9-e761-10c19f54f947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload(utils);\n",
        "utils.initialize_with_pretrained(pretrained_embeds,embedder)\n",
        "print(embedder.forward(['four'])[0][0,:5])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.1243, -0.1147, -0.5684, -0.3970,  0.2294], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZS1xjCnBs-K",
        "outputId": "c0bdfef1-0fd1-41d5-e939-f6ce8ab54b87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_pretrained_embeddings_d4_3"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.002s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtpNJKO3SO8l"
      },
      "source": [
        "### Deliverable 4.4: Better Arc Component Combination (3 points)\n",
        "Before, in order to combine two embeddings during an arc- operation, we just passed them through a feed-forward network and got a dense output.  Now, we will instead use a sequence model of the stack.  The combined embedding from an arc- operation is the next time step of an LSTM.  Implement `neural_net.LSTMCombiner`.\n",
        "\n",
        "**Test**: `tests/test_parser.py:test_lstm_combiner_d4_4`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "kmkJjbYqSO8m"
      },
      "source": [
        "reload(neural_net)\n",
        "torch.manual_seed(1)\n",
        "combiner = neural_net.LSTMCombiner(TEST_EMBEDDING_DIM,\n",
        "                                          num_layers=LSTM_NUM_LAYERS,\n",
        "                                          dropout=DROPOUT)\n",
        "head_feat = ag.Variable(torch.randn(1,TEST_EMBEDDING_DIM))\n",
        "mod_feat = ag.Variable(torch.randn(1,TEST_EMBEDDING_DIM))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "h50pk9XCSO8s",
        "outputId": "b3cdc109-d047-40a5-dad1-76a8ad2bddab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "combined = combiner(head_feat, mod_feat)\n",
        "combined"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0532, -0.1534,  0.1484, -0.0595]], grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmcu6xG8dsiz",
        "outputId": "5ec4c987-9891-417e-8a51-342173a4daa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_lstm_combiner_d4_4"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.004s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRxft7-qSO8x"
      },
      "source": [
        "### Deliverable 4.5: Better action choosing (3 points)\n",
        "Instead of choosing the action from the combiner output independently at each time step, let's use an LSTM to predict the action. This way, past actions can influence the current decision directly. \n",
        "\n",
        "Implement `neural_net.LSTMActionChooser`. Use a linear layer to predict the action from the LSTM hidden state.\n",
        "\n",
        "**Test**: `tests/test_parser.py:test_lstm_action_chooser_d4_5`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V5nkneYSO8z"
      },
      "source": [
        "reload(neural_net)\n",
        "torch.manual_seed(1)\n",
        "action_chooser = neural_net.LSTMActionChooser(TEST_EMBEDDING_DIM * NUM_FEATURES,\n",
        "                                                     LSTM_NUM_LAYERS,\n",
        "                                                     dropout=DROPOUT)\n",
        "feats = [ag.Variable(torch.randn(1, TEST_EMBEDDING_DIM)) for _ in range(NUM_FEATURES)]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "rmPOSefnSO83",
        "outputId": "482e8b29-26a6-42b7-b31d-800272df307d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output = action_chooser(feats)\n",
        "output"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.0328, -1.1798, -1.0887]], grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13jyS-qQI48O",
        "outputId": "16b9e029-bd8c-4d23-f34b-5bf4d67acc12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(feats)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[ 0.8458, -0.9786, -1.1070, -0.8447]]), tensor([[-0.6461,  0.7752,  1.6853, -0.0768]]), tensor([[-1.3098, -0.0978, -0.4916, -1.0949]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auIeWYmBoxM1"
      },
      "source": [
        "#tensor([[ 0.8458, -0.9786, -1.1070, -0.8447]]), tensor([[-0.6461,  0.7752,  1.6853, -0.0768]]), tensor([[-1.3098, -0.0978, -0.4916, -1.0949]])]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mdp1JPiJkah",
        "outputId": "1398a64d-e03c-4123-bf10-d4bb26cc1a5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_lstm_action_chooser_d4_5"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.005s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJhk19ECSO86"
      },
      "source": [
        "### Retrain with the new components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YswialxlSO87"
      },
      "source": [
        "reload(utils)\n",
        "reload(neural_net)\n",
        "reload(parsing)\n",
        "reload(feat_extractors)\n",
        "torch.manual_seed(1)\n",
        "stack_dim = STACK_EMBEDDING_DIM\n",
        "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
        "# BiLSTM word embeddings will probably work best, but feel free to experiment with the others you developed\n",
        "word_embedding_lookup = neural_net.BiLSTMWordEmbedding(word_to_ix_en,\n",
        "                                                       WORD_EMBEDDING_DIM,\n",
        "                                                       STACK_EMBEDDING_DIM,\n",
        "                                                       num_layers=LSTM_NUM_LAYERS,\n",
        "                                                       dropout=DROPOUT)\n",
        "utils.initialize_with_pretrained(pretrained_embeds, word_embedding_lookup)\n",
        "action_chooser = neural_net.LSTMActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES,\n",
        "                                              LSTM_NUM_LAYERS,\n",
        "                                              dropout=DROPOUT)\n",
        "combiner = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
        "                                   num_layers=LSTM_NUM_LAYERS,\n",
        "                                   dropout=DROPOUT)\n",
        "parser = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
        "                                  action_chooser, combiner)\n",
        "optimizer = optim.SGD(parser.parameters(), lr=ETA_0)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "awlmzoDkSO8-",
        "outputId": "ee9203d5-8065-4794-d70f-bb6b84ffa182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The LSTMs will make this take longer, probably just a few minutes\n",
        "train_parser(parser, optimizer, en_dataset, n_epochs=5, n_train_insts=1000)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Number of instances: 1000    Number of network actions: 44560\n",
            "Acc: 0.7976211849192101  Loss: 20.79837021105364\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.8580083301779629  Loss: 10.992653027965043\n",
            "F-Score: 0.5744709872655923\n",
            "Attachment Score: 0.5582481383314402\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Number of instances: 1000    Number of network actions: 44560\n",
            "Acc: 0.8857271095152603  Loss: 12.418924878502265\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.8794648491732929  Loss: 9.513310367267527\n",
            "F-Score: 0.6232499476204901\n",
            "Attachment Score: 0.6081029912911776\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Number of instances: 1000    Number of network actions: 44560\n",
            "Acc: 0.9114901256732495  Loss: 9.862190837427043\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.8873532752745172  Loss: 8.863051471467726\n",
            "F-Score: 0.6518197524297765\n",
            "Attachment Score: 0.6327148807269973\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Number of instances: 1000    Number of network actions: 44560\n",
            "Acc: 0.9289946140035906  Loss: 7.996838311217259\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.8925911902057302  Loss: 8.956554038708513\n",
            "F-Score: 0.6678507427032702\n",
            "Attachment Score: 0.6426858513189448\n",
            "\n",
            "\n",
            "Epoch 5\n",
            "Number of instances: 1000    Number of network actions: 44560\n",
            "Acc: 0.9432450628366248  Loss: 6.484751487516565\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.8942319828347848  Loss: 9.33465821182582\n",
            "F-Score: 0.6713497994518581\n",
            "Attachment Score: 0.6405401994194119\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LydtfN63SO9B"
      },
      "source": [
        "### Deliverable 4.6: Test Predictions: English (2 point)\n",
        "\n",
        "**Test**: `tests/test_parser.py:test_dev_preds_d4_6_english`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKagGPmOSO9D"
      },
      "source": [
        "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
        "evaluation.output_preds(consts.EN_D4_6_DEV_FILENAME, parser, dev_sentences)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIlO4IgZSO9H"
      },
      "source": [
        "evaluation.output_preds(consts.EN_D4_6_TEST_FILENAME, parser, en_dataset.test_data)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqsM8C7XLoxh",
        "outputId": "c06c976f-cb15-41e6-c242-f667cd06c378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_dev_preds_d4_6_english"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.008s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awqj0wV9SO9K"
      },
      "source": [
        "### Deliverable 4.7: Test Predictions: Norwegian (2 point)\n",
        "\n",
        "**Test**: `tests/test_parser.py:test_dev_preds_d4_7_norwegian`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "AmQlLT1iSO9L",
        "outputId": "ad07d0f1-b0c1-4f2a-9882-22f8a8e715f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.manual_seed(1)\n",
        "feat_extractor_nr = feat_extractors.SimpleFeatureExtractor()\n",
        "# BiLSTM word embeddings will probably work best, but feel free to experiment with the others you developed\n",
        "word_embedding_lookup_nr = neural_net.BiLSTMWordEmbedding(word_to_ix_nr,\n",
        "                                                          WORD_EMBEDDING_DIM,\n",
        "                                                          STACK_EMBEDDING_DIM,\n",
        "                                                          num_layers=LSTM_NUM_LAYERS,\n",
        "                                                          dropout=DROPOUT)\n",
        "action_chooser_nr = neural_net.FFActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES)\n",
        "combiner_nr = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
        "                                          num_layers=LSTM_NUM_LAYERS,\n",
        "                                          dropout=DROPOUT)\n",
        "parser_nr = parsing.TransitionParser(feat_extractor_nr, word_embedding_lookup_nr,\n",
        "                                  action_chooser_nr, combiner_nr)\n",
        "optimizer_nr = optim.SGD(parser_nr.parameters(), lr=ETA_0)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input dim:  300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true,
        "id": "5uj2-m3jSO9P",
        "outputId": "4c0a82fc-0349-4ffa-a232-d6edd22ed9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_parser(parser_nr, optimizer_nr, nr_dataset, n_epochs=3, n_train_insts=1000)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of instances: 1000    Number of network actions: 30942\n",
            "Acc: 0.8121970137676944  Loss: 13.314200128350407\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 16028\n",
            "Acc: 0.8414025455452957  Loss: 11.624937924874311\n",
            "F-Score: 0.5086633066653673\n",
            "Attachment Score: 0.48777140004991265\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Number of instances: 1000    Number of network actions: 30942\n",
            "Acc: 0.8828130049770538  Loss: 8.545607123799622\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 16028\n",
            "Acc: 0.8579361118043424  Loss: 10.674043948669173\n",
            "F-Score: 0.5503443019736685\n",
            "Attachment Score: 0.5272023958073372\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Number of instances: 1000    Number of network actions: 30942\n",
            "Acc: 0.9173615150927542  Loss: 6.272148632211843\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 16028\n",
            "Acc: 0.8632393311704517  Loss: 11.034085195957722\n",
            "F-Score: 0.5621443059272138\n",
            "Attachment Score: 0.5354379835288245\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVDjjwYmSO9X",
        "outputId": "b865103f-c9d8-4de7-b221-47842d756be0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
        "evaluation.output_preds(consts.NR_D4_7_DEV_FILENAME, parser_nr, dev_sentences_nr)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M67ucPnVSO9b",
        "outputId": "f5097787-39d8-4645-e92a-2debc1868fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluation.output_preds(consts.NR_D4_7_TEST_FILENAME, parser_nr, nr_dataset.test_data)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mynlplib/neural_net.py:428: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.log_softmax(out3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlpQ0IdGKdkE",
        "outputId": "81b36e68-4112-4670-e846-c3d4a9cca231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nosetests tests/test_parser.py:test_dev_preds_d4_7_norwegian"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.010s\n",
            "\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks_SbyseSO9f"
      },
      "source": [
        "# 5. Bakeoff (24 points)\n",
        "\n",
        "We will have another bakeoff for this problem set.\n",
        "\n",
        "Try to implement new features and tune your network's architecture and hyperparameters to get the best network.\n",
        "Section 3 of [this paper](https://arxiv.org/pdf/1206.5533.pdf) may help out with hyper parameter tuning if you are new to neural networks.\n",
        "To get very competitive, it may be necessary to train for a large amount of time (leaving it running overnight should be fine).  Here are some suggestions.\n",
        "* Tune your learning rate.\n",
        "* Tune your other hyperparameters.\n",
        "* Try different optimizers.  torch.optim has a ton of different training algorithms.  SGD was used in this pset because it is fast, but it is the most vanilla of them.  Trying new ones, like Adam, will almost certainly boost performance\n",
        "* Try adding regularization to your network if you see evidence that it is overfitting. This can be done with:\n",
        "  * L2 regularization using the [weight decay argument](http://pytorch.org/docs/master/optim.html#torch.optim.SGD)\n",
        "  * adding dropout (already an input argument to some of the neural net components)\n",
        "  * implement early stopping (stop training if dev performance on some metric doesn't improve for k epochs)\n",
        "* Try customizing any of the 3 components (word embeddings, action choosing, combining) in clever ways.  You can create new classes that expose the same public interface and use them here (just leave your required ones untouched). Building word embeddings from characters using an RNN or convolutional layer may help.\n",
        "* Try new features.  Write new classes that expose the same public interface as SimpleFeatureExtractor.  Try looking further into stack history, or more input buffer lookahead, or features based on the action sequence. The possibilities are endless.\n",
        "* Within our currect interface, you can use any neural nets and static pretrained word embeddings. However, if you use pretrained contextualized embeddings from large data (e.g., BERT), you will not receive extra credits if your model ranks high. You can still try creative modifications and get extra credits for \"creative solutions\".\n",
        "\n",
        "**Tests**: \n",
        "- `tests/test_parser.py:test_dev_preds_bakeoff_d5_1_english`\n",
        "- `tests/test_parser.py:test_dev_preds_bakeoff_d5_2_norwegian`\n",
        "\n",
        "**Rubric**:\n",
        "English dev:\n",
        "- $\\geq$ 0.76: 2 points\n",
        "- $\\geq$ 0.77: 4 points\n",
        "- $\\geq$ 0.78: 6 points\n",
        "\n",
        "English test:\n",
        "- $\\geq$ 0.72: 2 points\n",
        "- $\\geq$ 0.73: 4 points\n",
        "- $\\geq$ 0.74: 6 points\n",
        "\n",
        "Norwegian dev:\n",
        "- $\\geq$ 0.71: 2 points\n",
        "- $\\geq$ 0.72: 4 points\n",
        "- $\\geq$ 0.73: 6 points\n",
        "\n",
        "Norwegian test:\n",
        "- $\\geq$ 0.70: 2 points\n",
        "- $\\geq$ 0.71: 4 points\n",
        "- $\\geq$ 0.72: 6 points\n",
        "\n",
        "**Extra credit**:\n",
        "\n",
        "For both languages:\n",
        "- Top five test performance in the class (4 points bonus)\n",
        "- We'll also give 4 bonus points to particularly unique / creative / well-motivated solutions (with motivation to be included in the write-up)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fucUPojZSO9g"
      },
      "source": [
        "### Using Cuda\n",
        "You can use CUDA to train your network, and you should expect decent speedup if you have a GPU and the CUDA toolkit installed.\n",
        "If you want to use CUDA in this assignment, change the HAVE_CUDA variable to True in constants.py, and call `.to_cuda()` on your parser. You may also need to reconfigure your Embedding layers if you didn't consider cuda before.\n",
        "\n",
        "We are not officially supporting CUDA though. If you have problems installing or running CUDA, perhaps just use the CPU. You can post on Piazza but we cannot guarantee to help you debug it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfGEYwjmSO9k"
      },
      "source": [
        "# Set your hyperparameters here\n",
        "# e.g learning rate, regularization, lr annealing, dimensionality of embeddings, number of epochs, early stopping etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96mrhTFBFxkA"
      },
      "source": [
        "# def train_parser_bakeoff(parser, optimizer, dataset, scheduler, n_epochs=1, n_train_insts=1000):\n",
        "#     for epoch in range(n_epochs):\n",
        "#         print(\"Epoch {}\".format(epoch+1))\n",
        "\n",
        "#         parser.train() # turn on dropout layers if they are there\n",
        "#         parsing.train(dataset.training_data[:n_train_insts], parser, optimizer, verbose=True)\n",
        "\n",
        "#         print(\"Dev Evaluation\")\n",
        "#         parser.eval() # turn them off for evaluation\n",
        "#         parsing.evaluate(dataset.dev_data, parser, verbose=True)\n",
        "#         print(\"F-Score: {}\".format(evaluation.compute_metric(parser, dataset.dev_data, evaluation.fscore)))\n",
        "#         print(\"Attachment Score: {}\".format(evaluation.compute_attachment(parser, dataset.dev_data)))\n",
        "#         scheduler.step()\n",
        "#         print(\"\\n\")"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uriMfRpBSO9o"
      },
      "source": [
        "# # Make your parser here\n",
        "# # name your TransitionParser bakeoff_parser to output your predictions below\n",
        "# # bakeoff_parser_en = TransitionParser(...)\n",
        "\n",
        "# reload(neural_net)\n",
        "# reload(parsing)\n",
        "# torch.manual_seed(1)\n",
        "# stack_dim = STACK_EMBEDDING_DIM\n",
        "# feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
        "# word_embedding_lookup = neural_net.BiLSTMWordEmbedding(word_to_ix_en,\n",
        "#                                                        WORD_EMBEDDING_DIM,\n",
        "#                                                        STACK_EMBEDDING_DIM,\n",
        "#                                                        num_layers=LSTM_NUM_LAYERS,\n",
        "#                                                        dropout=DROPOUT)\n",
        "# utils.initialize_with_pretrained(pretrained_embeds, word_embedding_lookup)\n",
        "# action_chooser = neural_net.LSTMActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES,\n",
        "#                                               LSTM_NUM_LAYERS,\n",
        "#                                               dropout=DROPOUT)\n",
        "# combiner = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
        "#                                    num_layers=LSTM_NUM_LAYERS,\n",
        "#                                    dropout=DROPOUT)\n",
        "# bakeoff_parser_en = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
        "#                                   action_chooser, combiner)\n",
        "# bakeoff_optimizer_en = optim.Adam(bakeoff_parser_en.parameters(), lr = 0.01, weight_decay=0)#ETA_0lr=0.05->77.7\n",
        "# ##\n",
        "# bakeoff_scheduler = optim.lr_scheduler.StepLR(bakeoff_optimizer_en, step_size=2, gamma=0.01)\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6sxeE3rUcKq"
      },
      "source": [
        "reload(neural_net)\n",
        "reload(parsing)\n",
        "torch.manual_seed(1)\n",
        "stack_dim = STACK_EMBEDDING_DIM\n",
        "feat_extractor = feat_extractors.SimpleFeatureExtractor()\n",
        "word_embedding_lookup = neural_net.BiLSTMWordEmbedding(word_to_ix_en,\n",
        "                                                       WORD_EMBEDDING_DIM,\n",
        "                                                       STACK_EMBEDDING_DIM,\n",
        "                                                       num_layers=LSTM_NUM_LAYERS,\n",
        "                                                       dropout=DROPOUT)\n",
        "utils.initialize_with_pretrained(pretrained_embeds, word_embedding_lookup)\n",
        "action_chooser = neural_net.LSTMActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES,\n",
        "                                              LSTM_NUM_LAYERS,\n",
        "                                              dropout=DROPOUT)\n",
        "combiner = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
        "                                   num_layers=LSTM_NUM_LAYERS,\n",
        "                                   dropout=DROPOUT)\n",
        "bakeoff_parser_en = parsing.TransitionParser(feat_extractor, word_embedding_lookup,\n",
        "                                  action_chooser, combiner)\n",
        "bakeoff_optimizer_en = optim.SGD(bakeoff_parser_en.parameters(), lr=0.05)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O2lbnGwSO9t",
        "outputId": "9886518d-7cb6-497b-d1a1-37d3eecd8486",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train for bakeoff\n",
        "train_parser(bakeoff_parser_en, bakeoff_optimizer_en, en_dataset, n_epochs=10, n_train_insts=10000)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.8940557126879001  Loss: 8.639544051241758\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9175186166855989  Loss: 6.607166524007991\n",
            "F-Score: 0.7327193210169111\n",
            "Attachment Score: 0.7247254827716774\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.9363233373738793  Loss: 5.406476676498656\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9319702133030418  Loss: 5.551207159959085\n",
            "F-Score: 0.7800142422148288\n",
            "Attachment Score: 0.769657957844251\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.9495195263777384  Loss: 4.371362410224613\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9350624763347217  Loss: 5.3038992784813255\n",
            "F-Score: 0.7862490719179671\n",
            "Attachment Score: 0.7740754764609366\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.9590967915722408  Loss: 3.6273539247105857\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9343051874290041  Loss: 5.418627848033967\n",
            "F-Score: 0.7849587639845774\n",
            "Attachment Score: 0.7709201060204468\n",
            "\n",
            "\n",
            "Epoch 5\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.9662819364170312  Loss: 3.00023457655456\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9359459800580588  Loss: 5.720419441301008\n",
            "F-Score: 0.7836164909473495\n",
            "Attachment Score: 0.773318187555219\n",
            "\n",
            "\n",
            "Epoch 6\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.9733353243270149  Loss: 2.436954554001974\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9357566578316294  Loss: 6.196539069697319\n",
            "F-Score: 0.7919121967857187\n",
            "Attachment Score: 0.7771046320838066\n",
            "\n",
            "\n",
            "Epoch 7\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.9787871334961263  Loss: 1.979817911581494\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9355673356052  Loss: 6.809199783393453\n",
            "F-Score: 0.7853238662335226\n",
            "Attachment Score: 0.7692793133913921\n",
            "\n",
            "\n",
            "Epoch 8\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.9829506526360171  Loss: 1.5976672618129268\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9344314022466238  Loss: 7.5319840394212365\n",
            "F-Score: 0.7863366560665187\n",
            "Attachment Score: 0.773318187555219\n",
            "\n",
            "\n",
            "Epoch 9\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.9859254314307632  Loss: 1.3141979677813487\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9331692540704278  Loss: 8.211854248590999\n",
            "F-Score: 0.7819404232630194\n",
            "Attachment Score: 0.7672598763094788\n",
            "\n",
            "\n",
            "Epoch 10\n",
            "Number of instances: 10000    Number of network actions: 341538\n",
            "Acc: 0.987465523602059  Loss: 1.1473406694655601\n",
            "Dev Evaluation\n",
            "Number of instances: 501    Number of network actions: 15846\n",
            "Acc: 0.9346838318818629  Loss: 8.489672825676823\n",
            "F-Score: 0.7789162885308845\n",
            "Attachment Score: 0.7627161428751735\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zZYXMyzSO9w"
      },
      "source": [
        "# write dev output\n",
        "dev_sentences = [ sentence for sentence, _ in en_dataset.dev_data ]\n",
        "evaluation.output_preds(\"bakeoff-dev-en.preds\", bakeoff_parser_en, dev_sentences)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYHdx_ATSO9z",
        "outputId": "25a0b0a1-8431-4cd8-a43e-51aa6e9f2797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# dev performance\n",
        "evaluation.compute_output_attachment(\"bakeoff-dev-en.preds\", consts.EN_DEV_GOLD)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7627161428751735"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOqhuqz4SO95"
      },
      "source": [
        "# write test output\n",
        "evaluation.output_preds(\"bakeoff-test-en.preds\", bakeoff_parser_en, en_dataset.test_data)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE_I-kOg8hoY"
      },
      "source": [
        "#! nosetests tests/test_parser.py:test_dev_preds_bakeoff_d5_1_english"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlKllDT5SO9-"
      },
      "source": [
        "# Now make your norwegian parser if necessary\n",
        "# name your TransitionParser bakeoff_parser to output your predictions below\n",
        "# bakeoff_parser_nr = TransitionParser(...)\n",
        "\n",
        "# Also, choose an optimizer.\n",
        "# bakeoff_optimizer_nr = optim...."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHLYwYmC0GbA"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "reload(neural_net)\n",
        "reload(parsing)\n",
        "torch.manual_seed(1)\n",
        "torch.manual_seed(1)\n",
        "drop_out = 0.2\n",
        "lr_rate = 0.001\n",
        "feat_extractor_nr = feat_extractors.SimpleFeatureExtractor()\n",
        "word_embedding_lookup_nr = neural_net.BiLSTMWordEmbedding(word_to_ix_nr,\n",
        "                                                          WORD_EMBEDDING_DIM,\n",
        "                                                          STACK_EMBEDDING_DIM,\n",
        "                                                          num_layers=LSTM_NUM_LAYERS,\n",
        "                                                          dropout=drop_out)\n",
        "action_chooser_nr = neural_net.LSTMActionChooser(STACK_EMBEDDING_DIM * NUM_FEATURES,\n",
        "                                              LSTM_NUM_LAYERS,\n",
        "                                              dropout=drop_out)\n",
        "combiner_nr = neural_net.LSTMCombiner(STACK_EMBEDDING_DIM,\n",
        "                                          num_layers=LSTM_NUM_LAYERS,\n",
        "                                          dropout=drop_out)\n",
        "bakeoff_parser_nr = parsing.TransitionParser(feat_extractor_nr, word_embedding_lookup_nr,\n",
        "                                  action_chooser_nr, combiner_nr)\n",
        "bakeoff_optimizer_nr = optim.Adam (bakeoff_parser_nr.parameters(), lr=lr_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFEG5J-GSO-C"
      },
      "source": [
        "# train for bakeoff\n",
        "train_parser(bakeoff_parser_nr, bakeoff_optimizer_nr, nr_dataset, n_epochs=5, n_train_insts=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z6MD34iSO-G"
      },
      "source": [
        "# write dev output\n",
        "dev_sentences_nr = [ sentence for sentence, _ in nr_dataset.dev_data ]\n",
        "evaluation.output_preds(\"bakeoff-dev-nr.preds\", bakeoff_parser_nr, dev_sentences_nr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCuA3mr0SO-J"
      },
      "source": [
        "# dev performance\n",
        "evaluation.compute_output_attachment(\"bakeoff-dev-nr.preds\", consts.NR_DEV_GOLD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIbGz95KSO-L"
      },
      "source": [
        "# write test output\n",
        "evaluation.output_preds(\"bakeoff-test-nr.preds\", bakeoff_parser_nr, nr_dataset.test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oguvm9LZZ7RU"
      },
      "source": [
        "! nosetests tests/test_parser.py:test_dev_preds_bakeoff_d5_2_norwegian"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lPhXXHBSO-N"
      },
      "source": [
        "# 6. Writeup (22 points)\n",
        "\n",
        "You can start your write-up in any format you prefer (e.g., LaTeX, Markdown), but please remember to export to `pset3-writeup.pdf` upon submission. Also, you will be asked to post your Deliverable 6.2 and 6.3 on Piazza after the due date (plus late days).\n",
        "\n",
        "**Deliverable 6.1** (6 points):\n",
        "\n",
        "Consider a sentence of 10 words:\n",
        "- In Project 1, we learned to build a text classifier. If the classifier is binary, there are $2$ possible outputs for this sentence.\n",
        "- In Project 2, we learned to build a sequence tagger. If each word has four possible labels, there are $4^{10} = 1048576$ possible outputs for this sentence.\n",
        "- In this project, we learned to build a parser for **projective** dependency trees. **How many possible projective dependency trees are there for our sentence of 10 words? Why? Is this output space larger than the one above?**\n",
        "\n",
        "**Deliverable 6.2** (6 points):\n",
        "\n",
        "Briefly describe your bakeoff design.\n",
        "\n",
        "**Deliverable 6.3** (10 points):\n",
        "\n",
        "You will select a research paper at ACL, EMNLP or NAACL that **uses dependency trees for some downstream task**. Summarize the paper, answering the following questions:\n",
        "\n",
        "1. What is the task that is being solved?\n",
        "2. Briefly (one sentence) explain the metric for success on this task.\n",
        "3. Why are dependency features expected to help with this task?\n",
        "4. How are dependency features incorporated into the solution?\n",
        "5. Does the paper evaluate whether dependency features improve performance on the downstream task? If so, what is their impact? If not, why not?\n",
        "\n",
        "You must choose a paper in the main conference (not workshops). The paper must be at least four pages long. All papers from these conferences are available for free online: https://www.aclweb.org/anthology/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUsLiyFMSO-O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}